{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing/Exploding Gradients Problems\n",
    "## Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Activation Functions\n",
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2) # defaults to alpha=0.3\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "# [...] # more layers\n",
    "tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"), # no activation\n",
    "tf.keras.layers.LeakyReLU(alpha=0.2), # activation as a separate layer\n",
    "# [...] # more layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU and SELU\n",
    "### GELU, Swish, and Mish\n",
    "## Batch Normalization\n",
    "### Implementing batch normalization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers\n",
    "## Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1376/1376 [==============================] - 3s 2ms/step - loss: 1.1080 - accuracy: 0.6623 - val_loss: 0.6812 - val_accuracy: 0.7889\n",
      "Epoch 2/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.5910 - accuracy: 0.8137 - val_loss: 0.5084 - val_accuracy: 0.8363\n",
      "Epoch 3/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.4772 - accuracy: 0.8502 - val_loss: 0.4388 - val_accuracy: 0.8528\n",
      "Epoch 4/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.4188 - accuracy: 0.8666 - val_loss: 0.3958 - val_accuracy: 0.8669\n",
      "Epoch 5/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.3820 - accuracy: 0.8751 - val_loss: 0.3707 - val_accuracy: 0.8726\n",
      "Epoch 6/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.3566 - accuracy: 0.8816 - val_loss: 0.3516 - val_accuracy: 0.8752\n",
      "Epoch 7/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.3387 - accuracy: 0.8856 - val_loss: 0.3387 - val_accuracy: 0.8767\n",
      "Epoch 8/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.3245 - accuracy: 0.8894 - val_loss: 0.3326 - val_accuracy: 0.8822\n",
      "Epoch 9/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.3137 - accuracy: 0.8927 - val_loss: 0.3192 - val_accuracy: 0.8874\n",
      "Epoch 10/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.3041 - accuracy: 0.8956 - val_loss: 0.3156 - val_accuracy: 0.8869\n",
      "Epoch 11/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2957 - accuracy: 0.8991 - val_loss: 0.3095 - val_accuracy: 0.8887\n",
      "Epoch 12/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.9018 - val_loss: 0.3088 - val_accuracy: 0.8892\n",
      "Epoch 13/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2836 - accuracy: 0.9031 - val_loss: 0.2994 - val_accuracy: 0.8920\n",
      "Epoch 14/20\n",
      "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2775 - accuracy: 0.9051 - val_loss: 0.2919 - val_accuracy: 0.8952\n",
      "Epoch 15/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2727 - accuracy: 0.9067 - val_loss: 0.2887 - val_accuracy: 0.8960\n",
      "Epoch 16/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2681 - accuracy: 0.9084 - val_loss: 0.2856 - val_accuracy: 0.8962\n",
      "Epoch 17/20\n",
      "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2639 - accuracy: 0.9092 - val_loss: 0.2824 - val_accuracy: 0.8995\n",
      "Epoch 18/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2595 - accuracy: 0.9113 - val_loss: 0.2804 - val_accuracy: 0.8985\n",
      "Epoch 19/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2561 - accuracy: 0.9134 - val_loss: 0.2775 - val_accuracy: 0.9020\n",
      "Epoch 20/20\n",
      "1376/1376 [==============================] - 2s 2ms/step - loss: 0.2523 - accuracy: 0.9134 - val_loss: 0.2776 - val_accuracy: 0.9022\n",
      "INFO:tensorflow:Assets written to: my_model_A\\assets\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255\n",
    "\n",
    "\n",
    "pos_class_id = class_names.index(\"Pullover\")\n",
    "neg_class_id = class_names.index(\"T-shirt/top\")\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
    "    y_A = y[~y_for_B]\n",
    "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
    "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
    "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
    "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
    "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_A = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"my_model_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 51ms/step - loss: 0.7249 - accuracy: 0.5550 - val_loss: 0.7285 - val_accuracy: 0.5143\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.6928 - accuracy: 0.5550 - val_loss: 0.7014 - val_accuracy: 0.5143\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6704 - accuracy: 0.5550 - val_loss: 0.6833 - val_accuracy: 0.5143\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.6531 - accuracy: 0.5600 - val_loss: 0.6664 - val_accuracy: 0.5153\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.6379 - accuracy: 0.5600 - val_loss: 0.6495 - val_accuracy: 0.5223\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.6227 - accuracy: 0.5700 - val_loss: 0.6342 - val_accuracy: 0.5351\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.6081 - accuracy: 0.5950 - val_loss: 0.6201 - val_accuracy: 0.5618\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5954 - accuracy: 0.6250 - val_loss: 0.6075 - val_accuracy: 0.6083\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.5827 - accuracy: 0.6650 - val_loss: 0.5963 - val_accuracy: 0.6479\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.5714 - accuracy: 0.7250 - val_loss: 0.5854 - val_accuracy: 0.6904\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.5605 - accuracy: 0.7700 - val_loss: 0.5742 - val_accuracy: 0.7448\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.5490 - accuracy: 0.8000 - val_loss: 0.5627 - val_accuracy: 0.7804\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5374 - accuracy: 0.8350 - val_loss: 0.5528 - val_accuracy: 0.8051\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5270 - accuracy: 0.8550 - val_loss: 0.5429 - val_accuracy: 0.8318\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.5170 - accuracy: 0.8750 - val_loss: 0.5328 - val_accuracy: 0.8576\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5066 - accuracy: 0.9000 - val_loss: 0.5231 - val_accuracy: 0.8793\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.4966 - accuracy: 0.9050 - val_loss: 0.5139 - val_accuracy: 0.8922\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.4870 - accuracy: 0.9050 - val_loss: 0.5036 - val_accuracy: 0.9130\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.4769 - accuracy: 0.9250 - val_loss: 0.4944 - val_accuracy: 0.9219\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.4678 - accuracy: 0.9250 - val_loss: 0.4856 - val_accuracy: 0.9248\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4943 - accuracy: 0.9135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49426114559173584, 0.9135000109672546]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model_B = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 43ms/step - loss: 1.6804 - accuracy: 0.4400 - val_loss: 1.0825 - val_accuracy: 0.4837\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.8894 - accuracy: 0.4550 - val_loss: 0.6859 - val_accuracy: 0.5371\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.6390 - accuracy: 0.6300 - val_loss: 0.6286 - val_accuracy: 0.6568\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5991 - accuracy: 0.7150 - val_loss: 0.6204 - val_accuracy: 0.6677\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 42ms/step - loss: 0.5749 - accuracy: 0.7150 - val_loss: 0.5769 - val_accuracy: 0.7201\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.5330 - accuracy: 0.8300 - val_loss: 0.5248 - val_accuracy: 0.8170\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.4866 - accuracy: 0.8750 - val_loss: 0.4960 - val_accuracy: 0.8131\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.4494 - accuracy: 0.8750 - val_loss: 0.4624 - val_accuracy: 0.8655\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.4229 - accuracy: 0.8900 - val_loss: 0.4345 - val_accuracy: 0.8902\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.3961 - accuracy: 0.9150 - val_loss: 0.4103 - val_accuracy: 0.9100\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.3730 - accuracy: 0.9350 - val_loss: 0.3971 - val_accuracy: 0.9001\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.3616 - accuracy: 0.9450 - val_loss: 0.3749 - val_accuracy: 0.9219\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.3385 - accuracy: 0.9550 - val_loss: 0.3583 - val_accuracy: 0.9248\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.3237 - accuracy: 0.9550 - val_loss: 0.3444 - val_accuracy: 0.9288\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.3102 - accuracy: 0.9500 - val_loss: 0.3323 - val_accuracy: 0.9367\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2978 - accuracy: 0.9550 - val_loss: 0.3210 - val_accuracy: 0.9407\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2863 - accuracy: 0.9550 - val_loss: 0.3129 - val_accuracy: 0.9357\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2783 - accuracy: 0.9550 - val_loss: 0.3043 - val_accuracy: 0.9397\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2695 - accuracy: 0.9550 - val_loss: 0.2950 - val_accuracy: 0.9456\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2606 - accuracy: 0.9550 - val_loss: 0.2887 - val_accuracy: 0.9446\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/63 [==================>...........] - ETA: 0s - loss: 0.2988 - accuracy: 0.9359"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.9390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2984112501144409, 0.9390000104904175]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers\n",
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.optimizers.Adam with tf.keras.optimizers.Nadam,\n",
    "tf.keras.optimizers.Adamax, or\n",
    "tf.keras.optimizers.experimental.AdamW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1 ** (epoch / 20)\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1 ** (1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### piecewise constant scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "# history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 25\n",
    "n_steps = n_epochs * math.ceil(len(X_train) / batch_size)\n",
    "scheduled_learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=n_steps, decay_rate=0.1)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=scheduled_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization\n",
    "## $ℓ_1$ and $ℓ_2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\", \n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(tf.keras.layers.Dense, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_probas = np.stack([model(X_test, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.066, 0.11 , 0.123, 0.136, 0.073, 0.127, 0.08 , 0.068, 0.076,\n",
       "        0.14 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:1]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.069, 0.102, 0.119, 0.146, 0.066, 0.118, 0.083, 0.054, 0.087,\n",
       "       0.156], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba[0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.026, 0.037, 0.049, 0.067, 0.02 , 0.037, 0.041, 0.021, 0.046,\n",
       "       0.059], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "y_std[0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1764"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_proba.argmax(axis=1)\n",
    "accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs, training=False):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_constraint=tf.keras.constraints.max_norm(1.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
